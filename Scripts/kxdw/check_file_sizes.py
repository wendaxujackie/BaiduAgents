#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ‰«æCSVæ–‡ä»¶ï¼Œæ£€æŸ¥å·²ä¸‹è½½æ–‡ä»¶çš„å¤§å°
å¦‚æœæ–‡ä»¶å¤§å°å°äºè¯¦æƒ…é¡µçš„æ–‡ä»¶å¤§å°ï¼Œåˆ é™¤æ–‡ä»¶å¤¹å¹¶æ›´æ–°CSV
"""

import csv
import re
import sys
import time
from pathlib import Path
from typing import Optional, Dict

try:
    import requests
except ImportError:
    requests = None

try:
    from bs4 import BeautifulSoup
except ImportError:
    BeautifulSoup = None

# å¯¼å…¥ç™¾åº¦å»ºè®®è¯åŠŸèƒ½
try:
    baidu_suggestion_path = Path(__file__).parent.parent / 'web_download_for_duduo' / 'baidu_suggestion.py'
    if baidu_suggestion_path.exists():
        sys.path.insert(0, str(baidu_suggestion_path.parent))
        from baidu_suggestion import get_baidu_suggestions
    else:
        from baidu_suggestion import get_baidu_suggestions
except ImportError:
    print("âš ï¸  æ— æ³•å¯¼å…¥ baidu_suggestionï¼Œå°†ä½¿ç”¨æ¸¸æˆåç§°ä½œä¸ºæ–‡ä»¶å¤¹å")
    get_baidu_suggestions = None


def get_folder_name(game_name: str) -> str:
    """ä½¿ç”¨ç™¾åº¦å»ºè®®è¯è·å–æ–‡ä»¶å¤¹å"""
    if get_baidu_suggestions:
        try:
            suggestions = get_baidu_suggestions(game_name)
            if suggestions:
                folder_name = suggestions[0]
                folder_name = re.sub(r'[<>:"/\\|?*]', '_', folder_name)
                return folder_name
        except Exception as e:
            print(f"âš ï¸  è·å–å»ºè®®è¯å¤±è´¥: {e}ï¼Œä½¿ç”¨æ¸¸æˆåç§°")
    
    # å¦‚æœæ²¡æœ‰å»ºè®®è¯ï¼Œä½¿ç”¨æ¸¸æˆåç§°
    folder_name = re.sub(r'[<>:"/\\|?*]', '_', game_name)
    return folder_name


def parse_size_to_mb(size_str: str) -> float:
    """å°†å¤§å°å­—ç¬¦ä¸²è½¬æ¢ä¸ºMBæ•°å€¼"""
    if not size_str:
        return 0.0
    
    size_str = size_str.strip().upper()
    
    # åŒ¹é…æ•°å­—å’Œå•ä½
    match = re.match(r'(\d+\.?\d*)\s*([MG]B?)', size_str)
    if not match:
        return 0.0
    
    value = float(match.group(1))
    unit = match.group(2) or 'M'
    
    # è½¬æ¢ä¸ºMB
    if 'G' in unit:
        value = value * 1024
    
    return value


def get_file_size_from_page(page_url: str) -> Optional[float]:
    """ä»è¯¦æƒ…é¡µçš„ ul.azgm_txtList ä¸­è§£ææ–‡ä»¶å¤§å°"""
    if not requests:
        print("   âš ï¸  requestsæœªå®‰è£…ï¼Œæ— æ³•è§£æè¯¦æƒ…é¡µ")
        return None
    
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Referer': 'https://www.kxdw.com/'
        }
        
        response = requests.get(page_url, headers=headers, timeout=30)
        response.raise_for_status()
        response.encoding = 'utf-8'
        html = response.text
        
        # ä½¿ç”¨BeautifulSoupè§£æï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if BeautifulSoup:
            soup = BeautifulSoup(html, 'html.parser')
            ul = soup.find('ul', class_='azgm_txtList')
            if ul:
                lis = ul.find_all('li')
                for li in lis:
                    text = li.get_text(strip=True)
                    # æŸ¥æ‰¾åŒ…å«å¤§å°ä¿¡æ¯çš„liï¼ˆé€šå¸¸åŒ…å«"MB"æˆ–"GB"ï¼‰
                    if 'MB' in text.upper() or 'GB' in text.upper():
                        # æå–å¤§å°ä¿¡æ¯
                        size_match = re.search(r'(\d+\.?\d*)\s*([MG]B)', text, re.IGNORECASE)
                        if size_match:
                            value = float(size_match.group(1))
                            unit = size_match.group(2).upper()
                            if 'G' in unit:
                                value = value * 1024
                            return value
        
        # å¦‚æœBeautifulSoupä¸å¯ç”¨ï¼Œä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼
        # æŸ¥æ‰¾ ul class="azgm_txtList" åŠå…¶å†…å®¹
        ul_pattern = r'<ul[^>]*class=["\']azgm_txtList["\'][^>]*>(.*?)</ul>'
        ul_match = re.search(ul_pattern, html, re.DOTALL | re.IGNORECASE)
        if ul_match:
            ul_content = ul_match.group(1)
            # æŸ¥æ‰¾æ‰€æœ‰liæ ‡ç­¾
            li_pattern = r'<li[^>]*>(.*?)</li>'
            li_matches = re.findall(li_pattern, ul_content, re.DOTALL | re.IGNORECASE)
            for li_content in li_matches:
                # ç§»é™¤HTMLæ ‡ç­¾ï¼Œåªä¿ç•™æ–‡æœ¬
                text = re.sub(r'<[^>]+>', '', li_content).strip()
                # æŸ¥æ‰¾åŒ…å«å¤§å°ä¿¡æ¯çš„æ–‡æœ¬
                if 'MB' in text.upper() or 'GB' in text.upper():
                    size_match = re.search(r'(\d+\.?\d*)\s*([MG]B)', text, re.IGNORECASE)
                    if size_match:
                        value = float(size_match.group(1))
                        unit = size_match.group(2).upper()
                        if 'G' in unit:
                            value = value * 1024
                        return value
        
        return None
        
    except Exception as e:
        print(f"   âš ï¸  è§£æè¯¦æƒ…é¡µå¤±è´¥: {e}")
        return None


def check_and_cleanup_files(csv_file: str, download_dir: str = "./downloads", 
                            start: int = 0, limit: int = None):
    """æ‰«æCSVæ–‡ä»¶ï¼Œæ£€æŸ¥æ–‡ä»¶å¤§å°å¹¶æ¸…ç†ä¸å®Œæ•´çš„æ–‡ä»¶
    
    Args:
        csv_file: CSVæ–‡ä»¶è·¯å¾„
        download_dir: ä¸‹è½½ç›®å½•
        start: ä»ç¬¬å‡ æ¡å¼€å§‹ï¼ˆä»0å¼€å§‹ï¼‰
        limit: æ£€æŸ¥çš„æ•°é‡é™åˆ¶ï¼ˆNoneè¡¨ç¤ºæ£€æŸ¥æ‰€æœ‰ï¼‰
    """
    csv_path = Path(csv_file)
    download_path = Path(download_dir)
    
    if not csv_path.exists():
        print(f"âŒ CSVæ–‡ä»¶ä¸å­˜åœ¨: {csv_file}")
        return
    
    if not download_path.exists():
        print(f"âŒ ä¸‹è½½ç›®å½•ä¸å­˜åœ¨: {download_dir}")
        return
    
    # è¯»å–CSVæ–‡ä»¶
    all_games = []
    with open(csv_path, 'r', encoding='utf-8-sig') as f:
        reader = csv.DictReader(f)
        for row in reader:
            all_games.append(row)
    
    total_count = len(all_games)
    print(f"ğŸ“‹ å…± {total_count} æ¡è®°å½•")
    
    # åº”ç”¨startå’Œlimitå‚æ•°
    if start < 0:
        start = 0
    if start >= total_count:
        print(f"âŒ startå‚æ•° ({start}) è¶…å‡ºæ€»è®°å½•æ•° ({total_count})")
        return
    
    games = all_games[start:]
    if limit is not None and limit > 0:
        games = games[:limit]
    
    end_index = start + len(games) - 1
    print(f"ğŸ“‹ å°†æ£€æŸ¥ç¬¬ {start + 1} åˆ° {end_index + 1} æ¡è®°å½•ï¼ˆå…± {len(games)} æ¡ï¼‰")
    print(f"{'='*60}")
    
    updated_count = 0
    deleted_count = 0
    skipped_count = 0
    
    for i, game in enumerate(games):
        # è®¡ç®—å®é™…ç´¢å¼•ï¼ˆä»1å¼€å§‹ï¼Œè€ƒè™‘startåç§»ï¼‰
        actual_index = start + i + 1
        total_to_check = len(games)
        
        game_name = game.get('æ¸¸æˆåç§°', '').strip()
        page_url = game.get('è¯¦æƒ…é¡µé“¾æ¥', '').strip()
        
        if not game_name or not page_url:
            print(f"\n[{actual_index}/{total_count}] âš ï¸  è·³è¿‡ï¼šæ¸¸æˆåç§°æˆ–è¯¦æƒ…é¡µé“¾æ¥ä¸ºç©º")
            skipped_count += 1
            continue
        
        print(f"\n[{actual_index}/{total_count}] ğŸ” æ£€æŸ¥: {game_name}")
        
        # 1. è·å–æ–‡ä»¶å¤¹å
        folder_name = get_folder_name(game_name)
        folder_path = download_path / folder_name
        
        # 2. æ£€æŸ¥æ–‡ä»¶å¤¹æ˜¯å¦å­˜åœ¨
        if not folder_path.exists() or not folder_path.is_dir():
            print(f"   â­ï¸  æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {folder_name}ï¼Œè·³è¿‡")
            skipped_count += 1
            continue
        
        # 3. è§£æè¯¦æƒ…é¡µè·å–æ–‡ä»¶å¤§å°ï¼ˆä» ul.azgm_txtList ä¸­æå–ï¼‰
        print(f"   ğŸ“„ è§£æè¯¦æƒ…é¡µ: {page_url}")
        expected_size_mb = get_file_size_from_page(page_url)
        
        if expected_size_mb is None or expected_size_mb == 0:
            print(f"   âš ï¸  æ— æ³•ä»è¯¦æƒ…é¡µè·å–æ–‡ä»¶å¤§å°ï¼Œè·³è¿‡")
            skipped_count += 1
            continue
        
        print(f"   ğŸ“Š è¯¦æƒ…é¡µæ–‡ä»¶å¤§å°: {expected_size_mb:.2f}MB")
        
        # 4. æ£€æŸ¥æ–‡ä»¶å¤¹ä¸­çš„æ–‡ä»¶
        files = [f for f in folder_path.iterdir() if f.is_file()]
        if not files:
            print(f"   âš ï¸  æ–‡ä»¶å¤¹ä¸ºç©ºï¼Œè·³è¿‡")
            skipped_count += 1
            continue
        
        # æŸ¥æ‰¾æœ€å¤§çš„æ–‡ä»¶ï¼ˆé€šå¸¸æ˜¯APKæ–‡ä»¶ï¼‰
        largest_file = max(files, key=lambda f: f.stat().st_size)
        existing_file_size_bytes = largest_file.stat().st_size
        existing_file_size_mb = existing_file_size_bytes / 1024 / 1024
        
        print(f"   ğŸ“„ æ‰¾åˆ°æ–‡ä»¶: {largest_file.name} ({existing_file_size_mb:.2f}MB)")
        
        # 5. æ¯”è¾ƒæ–‡ä»¶å¤§å°
        # ä½¿ç”¨5%çš„å®¹å·®ï¼ˆè€ƒè™‘æµ®ç‚¹æ•°ç²¾åº¦å’Œæ–‡ä»¶ç³»ç»Ÿå·®å¼‚ï¼‰
        # åªè¦æ–‡ä»¶å¤§å° >= è¯¦æƒ…é¡µå¤§å° * 0.95ï¼Œå°±è®¤ä¸ºæ–‡ä»¶å®Œæ•´
        min_acceptable_size = expected_size_mb * 0.95
        if existing_file_size_mb >= min_acceptable_size:
            print(f"   âœ… æ–‡ä»¶å¤§å°å®Œæ•´: {existing_file_size_mb:.2f}MB >= {min_acceptable_size:.2f}MB (è¯¦æƒ…é¡µ: {expected_size_mb:.2f}MB, å®¹å·®5%)")
        else:
            size_diff = existing_file_size_mb - expected_size_mb
            print(f"   âš ï¸  æ–‡ä»¶å¤§å°ä¸å®Œæ•´: {existing_file_size_mb:.2f}MB < {min_acceptable_size:.2f}MB (è¯¦æƒ…é¡µ: {expected_size_mb:.2f}MB, å·®å¼‚: {size_diff:.2f}MB)")
            print(f"   ğŸ—‘ï¸  åˆ é™¤æ–‡ä»¶å¤¹ä¸‹çš„æ‰€æœ‰æ–‡ä»¶...")
            
            # åˆ é™¤æ–‡ä»¶å¤¹ä¸‹çš„æ‰€æœ‰æ–‡ä»¶
            try:
                for file in files:
                    try:
                        file.unlink()
                        print(f"      âœ… å·²åˆ é™¤: {file.name}")
                    except Exception as e:
                        print(f"      âš ï¸  åˆ é™¤å¤±è´¥ {file.name}: {e}")
                
                # æ›´æ–°CSV
                game['æ˜¯å¦å·²ä¸‹è½½'] = 'å¦'
                updated_count += 1
                deleted_count += 1
                print(f"   âœ… å·²æ›´æ–°CSV: æ˜¯å¦å·²ä¸‹è½½ = å¦")
            except Exception as e:
                print(f"   âŒ åˆ é™¤æ–‡ä»¶æ—¶å‡ºé”™: {e}")
    
    # ä¿å­˜CSVæ–‡ä»¶ï¼ˆéœ€è¦æ›´æ–°åŸå§‹CSVæ–‡ä»¶ï¼‰
    if updated_count > 0:
        print(f"\n{'='*60}")
        print(f"ğŸ’¾ ä¿å­˜CSVæ–‡ä»¶...")
        # æ›´æ–°all_gamesä¸­å¯¹åº”çš„è®°å½•
        for i, game in enumerate(games):
            original_index = start + i
            if original_index < len(all_games):
                all_games[original_index] = game
        
        with open(csv_path, 'w', newline='', encoding='utf-8-sig') as f:
            if all_games:
                fieldnames = list(all_games[0].keys())
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                writer.writeheader()
                writer.writerows(all_games)
        print(f"âœ… CSVæ–‡ä»¶å·²ä¿å­˜")
    
    # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
    print(f"\n{'='*60}")
    print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    print(f"   æ€»è®°å½•æ•°: {total_count}")
    print(f"   æ£€æŸ¥èŒƒå›´: ç¬¬ {start + 1} åˆ° {end_index + 1} æ¡ï¼ˆå…± {len(games)} æ¡ï¼‰")
    print(f"   å·²æ›´æ–°: {updated_count}")
    print(f"   å·²åˆ é™¤æ–‡ä»¶å¤¹: {deleted_count}")
    print(f"   è·³è¿‡: {skipped_count}")
    print(f"{'='*60}")


if __name__ == '__main__':
    import argparse
    
    parser = argparse.ArgumentParser(description='æ‰«æCSVæ–‡ä»¶ï¼Œæ£€æŸ¥å·²ä¸‹è½½æ–‡ä»¶çš„å¤§å°')
    parser.add_argument('csv_file', help='CSVæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--download-dir', default='./downloads', help='ä¸‹è½½ç›®å½•ï¼ˆé»˜è®¤: ./downloadsï¼‰')
    parser.add_argument('--start', type=int, default=0, help='ä»ç¬¬å‡ æ¡å¼€å§‹ï¼ˆä»0å¼€å§‹ï¼Œé»˜è®¤: 0ï¼‰')
    parser.add_argument('--limit', type=int, default=None, help='æ£€æŸ¥çš„æ•°é‡é™åˆ¶ï¼ˆé»˜è®¤: æ£€æŸ¥æ‰€æœ‰ï¼‰')
    
    args = parser.parse_args()
    
    check_and_cleanup_files(
        csv_file=args.csv_file,
        download_dir=args.download_dir,
        start=args.start,
        limit=args.limit
    )

