# -*- coding: utf-8 -*-
"""
æ¸¸æˆåç§°è¯†åˆ«æ¨¡å—
ä½¿ç”¨OCRå’Œè‡ªç„¶è¯­è¨€å¤„ç†ä»æˆªå›¾ä¸­æå–æ¸¸æˆåç§°
æ”¯æŒç½‘ç»œæœç´¢éªŒè¯æ¸¸æˆåç§°
"""
import re
import time
from typing import List, Dict, Any, Optional, Set
from pathlib import Path
from urllib.parse import quote

from loguru import logger
import jieba
import jieba.analyse
import requests

# å°è¯•å¯¼å…¥PaddleOCRï¼ˆæ¨èï¼‰
try:
    from paddleocr import PaddleOCR
    HAS_PADDLE_OCR = True
except ImportError:
    HAS_PADDLE_OCR = False
    logger.warning("PaddleOCRæœªå®‰è£…ï¼Œå°†å°è¯•ä½¿ç”¨å…¶ä»–OCRæ–¹æ¡ˆ")

# å°è¯•å¯¼å…¥Tesseract
try:
    import pytesseract
    from PIL import Image
    HAS_TESSERACT = True
except ImportError:
    HAS_TESSERACT = False

# å°è¯•å¯¼å…¥Mac Visionæ¡†æ¶ï¼ˆmacOSåŸç”ŸOCRï¼‰
HAS_VISION = False
try:
    import platform
    if platform.system() == 'Darwin':  # macOS
        import Vision
        import Quartz
        from Foundation import NSURL
        HAS_VISION = True
        logger.info("âœ… Mac Vision OCR å¯ç”¨")
except ImportError:
    pass

import sys
sys.path.append(str(Path(__file__).parent.parent))
from config import OCR_CONFIG, GAME_KEYWORDS, EXCLUDE_KEYWORDS, GAMES_CSV_PATH


class GameRecognizer:
    """æ¸¸æˆåç§°è¯†åˆ«å™¨"""
    
    def __init__(self, use_paddle: bool = True):
        """
        åˆå§‹åŒ–æ¸¸æˆåç§°è¯†åˆ«å™¨
        
        Args:
            use_paddle: æ˜¯å¦ä¼˜å…ˆä½¿ç”¨PaddleOCR
        """
        self.ocr_engine = None
        self.use_paddle = use_paddle and HAS_PADDLE_OCR
        
        # å·²è¯†åˆ«çš„æ¸¸æˆåç§°ç¼“å­˜
        self.recognized_games: Set[str] = set()
        
        # åˆå§‹åŒ–OCRå¼•æ“
        self._init_ocr()
        
        # åŠ è½½å·²æœ‰çš„æ¸¸æˆæ•°æ®
        self._load_existing_games()
    
    def _init_ocr(self):
        """åˆå§‹åŒ–OCRå¼•æ“"""
        if self.use_paddle:
            try:
                # ä½¿ç”¨æœ€ç®€å‚æ•°åˆå§‹åŒ–ï¼ˆå…¼å®¹æ‰€æœ‰ç‰ˆæœ¬ï¼‰
                self.ocr_engine = PaddleOCR(
                    use_angle_cls=True,
                    lang="ch",
                )
                logger.info("PaddleOCRå¼•æ“åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                logger.error(f"PaddleOCRåˆå§‹åŒ–å¤±è´¥: {e}")
                self.use_paddle = False
        
        if not self.use_paddle and HAS_TESSERACT:
            logger.info("ä½¿ç”¨Tesseractä½œä¸ºOCRå¼•æ“")
        elif not self.use_paddle and not HAS_TESSERACT:
            logger.warning("æ²¡æœ‰å¯ç”¨çš„OCRå¼•æ“ï¼ŒOCRåŠŸèƒ½å°†è¢«ç¦ç”¨")
    
    def _load_existing_games(self):
        """ä»CSVæ–‡ä»¶åŠ è½½å·²æœ‰çš„æ¸¸æˆåç§°"""
        try:
            if GAMES_CSV_PATH.exists():
                import pandas as pd
                df = pd.read_csv(GAMES_CSV_PATH)
                if 'game_name' in df.columns:
                    self.recognized_games = set(df['game_name'].dropna().tolist())
                    logger.info(f"ä»CSVåŠ è½½äº† {len(self.recognized_games)} ä¸ªå·²è¯†åˆ«çš„æ¸¸æˆ")
        except Exception as e:
            logger.warning(f"åŠ è½½å·²æœ‰æ¸¸æˆæ•°æ®æ—¶å‡ºé”™: {e}")
    
    def ocr_with_mac_vision(self, image_path: Path) -> List[str]:
        """
        ä½¿ç”¨MacåŸç”ŸVisionæ¡†æ¶è¿›è¡ŒOCRï¼ˆæ•ˆæœæ›´å¥½ï¼Œæ–‡å­—æ›´è¿è´¯ï¼‰
        
        Args:
            image_path: å›¾ç‰‡è·¯å¾„
            
        Returns:
            è¯†åˆ«å‡ºçš„æ–‡æœ¬åˆ—è¡¨
        """
        if not HAS_VISION:
            return []
        
        texts = []
        try:
            # åŠ è½½å›¾ç‰‡
            image_url = NSURL.fileURLWithPath_(str(image_path))
            ci_image = Quartz.CIImage.imageWithContentsOfURL_(image_url)
            
            if ci_image is None:
                logger.warning(f"Visionæ— æ³•åŠ è½½å›¾ç‰‡: {image_path}")
                return []
            
            # åˆ›å»ºæ–‡å­—è¯†åˆ«è¯·æ±‚
            request = Vision.VNRecognizeTextRequest.alloc().init()
            request.setRecognitionLevel_(Vision.VNRequestTextRecognitionLevelAccurate)
            request.setRecognitionLanguages_(['zh-Hans', 'zh-Hant', 'en'])  # ç®€ä½“ä¸­æ–‡ã€ç¹ä½“ä¸­æ–‡ã€è‹±æ–‡
            request.setUsesLanguageCorrection_(True)
            
            # åˆ›å»ºå¤„ç†å™¨å¹¶æ‰§è¡Œ
            handler = Vision.VNImageRequestHandler.alloc().initWithCIImage_options_(ci_image, None)
            success = handler.performRequests_error_([request], None)
            
            if success and request.results():
                for observation in request.results():
                    # è·å–è¯†åˆ«çš„æ–‡æœ¬ï¼ˆç½®ä¿¡åº¦æœ€é«˜çš„å€™é€‰ï¼‰
                    if observation.topCandidates_(1):
                        text = observation.topCandidates_(1)[0].string()
                        confidence = observation.confidence()
                        if text and confidence > 0.5:
                            texts.append(text.strip())
                
                if texts:
                    logger.info(f"ğŸ Mac Visionè¯†åˆ«å‡º {len(texts)} æ¡æ–‡æœ¬:")
                    for t in texts:
                        logger.info(f"  ğŸ“ {t}")
            
        except Exception as e:
            logger.warning(f"Mac Vision OCRå¤±è´¥: {e}")
        
        return texts
    
    def ocr_image(self, image_path: Path) -> List[str]:
        """
        å¯¹å›¾ç‰‡è¿›è¡ŒOCRè¯†åˆ«ï¼ˆåˆå¹¶Mac Visionå’ŒPaddleOCRçš„ç»“æœï¼‰
        
        Args:
            image_path: å›¾ç‰‡è·¯å¾„
            
        Returns:
            è¯†åˆ«å‡ºçš„æ–‡æœ¬åˆ—è¡¨
        """
        if not image_path.exists():
            logger.error(f"å›¾ç‰‡ä¸å­˜åœ¨: {image_path}")
            return []
        
        all_texts = []
        
        # æ–¹æ³•1: Mac Vision OCRï¼ˆå¯¹å¸¦ç‰¹æ•ˆæ–‡å­—æ•ˆæœä¸å¥½ï¼Œæš‚æ—¶ç¦ç”¨ï¼‰
        # if HAS_VISION:
        #     vision_texts = self.ocr_with_mac_vision(image_path)
        #     if vision_texts:
        #         all_texts.extend(vision_texts)
        
        texts = []
        
        # æ–¹æ³•2: PaddleOCRï¼ˆè¯†åˆ«ç‡æ›´é«˜ï¼‰
        if self.use_paddle and self.ocr_engine:
            try:
                result = self.ocr_engine.ocr(str(image_path))
                if result:
                    for page in result:
                        # æ–°ç‰ˆPaddleOCRè¿”å›å­—å…¸æ ¼å¼
                        if isinstance(page, dict):
                            rec_texts = page.get('rec_texts', [])
                            rec_scores = page.get('rec_scores', [])
                            for i, text in enumerate(rec_texts):
                                score = rec_scores[i] if i < len(rec_scores) else 1.0
                                if score > 0.5 and text.strip():
                                    texts.append(text.strip())
                        # æ—§ç‰ˆPaddleOCRè¿”å›åˆ—è¡¨æ ¼å¼
                        elif isinstance(page, list):
                            for line in page:
                                try:
                                    if isinstance(line, list) and len(line) >= 2:
                                        text_info = line[1]
                                        if isinstance(text_info, (tuple, list)):
                                            text = str(text_info[0])
                                            confidence = float(text_info[1]) if len(text_info) > 1 else 1.0
                                        else:
                                            text = str(text_info)
                                            confidence = 1.0
                                        if confidence > 0.5 and text.strip():
                                            texts.append(text.strip())
                                except Exception:
                                    continue
                
                # è¿‡æ»¤æ‰æ— ç”¨æ–‡æœ¬ï¼ˆæ•™ç¨‹ç±»ï¼‰
                filter_keywords = ['æ•™ç¨‹', 'å®‰è£…æ•™ç¨‹', 'æœºç‰ˆå®‰è£…', 'æ”»ç•¥', 'ç¤¼åŒ…ç ']
                filtered_texts = []
                for t in texts:
                    if not any(kw in t for kw in filter_keywords):
                        filtered_texts.append(t)
                    else:
                        logger.debug(f"  ğŸš« è¿‡æ»¤æ‰: {t}")
                texts = filtered_texts
                
                # è¾“å‡ºè¯†åˆ«ç»“æœåˆ°æ—¥å¿—
                if texts:
                    logger.info(f"OCRè¯†åˆ«å‡º {len(texts)} æ¡æœ‰æ•ˆæ–‡æœ¬:")
                    for t in texts:
                        logger.info(f"  ğŸ“ {t}")
                else:
                    logger.debug("OCRæœªè¯†åˆ«å‡ºæœ‰æ•ˆæ–‡æœ¬")
                    
            except Exception as e:
                logger.error(f"PaddleOCRè¯†åˆ«å¤±è´¥: {e}")
        
        elif HAS_TESSERACT:
            try:
                image = Image.open(image_path)
                text = pytesseract.image_to_string(image, lang='chi_sim+eng')
                texts = [line.strip() for line in text.split('\n') if line.strip()]
                logger.debug(f"Tesseractè¯†åˆ«å‡º {len(texts)} æ¡æ–‡æœ¬")
            except Exception as e:
                logger.error(f"Tesseractè¯†åˆ«å¤±è´¥: {e}")
        
        # åˆå¹¶æ‰€æœ‰OCRç»“æœå¹¶å»é‡
        all_texts.extend(texts)
        
        # æ™ºèƒ½åˆå¹¶ï¼šå¦‚æœæœ‰è¿ç»­çš„æ–‡å­—ï¼ˆå¦‚"çº¢æ¥¼æ¢¦galgame"ï¼‰ï¼Œä¼˜å…ˆä¿ç•™
        merged_texts = self._merge_ocr_texts(all_texts)
        
        return merged_texts
    
    def _merge_ocr_texts(self, texts: List[str]) -> List[str]:
        """
        æ™ºèƒ½åˆå¹¶OCRç»“æœï¼Œå¤„ç†åˆ†æ•£çš„æ–‡å­—
        ä¾‹å¦‚ï¼š['çº¢æ¥¼æ¢¦', 'galgame'] -> ['çº¢æ¥¼æ¢¦galgame']
        """
        if not texts:
            return []
        
        # å»é‡
        unique_texts = list(dict.fromkeys(texts))
        
        # è¿‡æ»¤æ— ç”¨æ–‡æœ¬
        filter_keywords = ['æ•™ç¨‹', 'å®‰è£…æ•™ç¨‹', 'æœºç‰ˆå®‰è£…', 'æ”»ç•¥', 'ç¤¼åŒ…ç ']
        filtered = [t for t in unique_texts if not any(kw in t for kw in filter_keywords)]
        
        # å°è¯•åˆå¹¶ç›¸é‚»çš„ä¸­è‹±æ–‡ï¼ˆå¦‚"çº¢æ¥¼æ¢¦" + "galgame"ï¼‰
        merged = []
        skip_next = set()
        
        for i, text in enumerate(filtered):
            if i in skip_next:
                continue
            
            # æ£€æŸ¥æ˜¯å¦å¯ä»¥å’Œä¸‹ä¸€ä¸ªæ–‡æœ¬åˆå¹¶
            if i + 1 < len(filtered):
                next_text = filtered[i + 1]
                
                # å¦‚æœå½“å‰æ˜¯ä¸­æ–‡ï¼Œä¸‹ä¸€ä¸ªæ˜¯è‹±æ–‡ï¼Œå¯èƒ½éœ€è¦åˆå¹¶
                if self._is_chinese(text) and self._is_english(next_text):
                    merged.append(text + next_text)
                    skip_next.add(i + 1)
                    continue
                
                # å¦‚æœå½“å‰æ˜¯è‹±æ–‡ï¼Œä¸‹ä¸€ä¸ªæ˜¯ä¸­æ–‡
                if self._is_english(text) and self._is_chinese(next_text):
                    merged.append(text + next_text)
                    skip_next.add(i + 1)
                    continue
            
            merged.append(text)
        
        return merged
    
    def _is_chinese(self, text: str) -> bool:
        """åˆ¤æ–­æ–‡æœ¬æ˜¯å¦ä¸»è¦æ˜¯ä¸­æ–‡"""
        chinese_count = sum(1 for c in text if '\u4e00' <= c <= '\u9fff')
        return chinese_count > len(text) * 0.5
    
    def _is_english(self, text: str) -> bool:
        """åˆ¤æ–­æ–‡æœ¬æ˜¯å¦ä¸»è¦æ˜¯è‹±æ–‡"""
        english_count = sum(1 for c in text if c.isascii() and c.isalpha())
        return english_count > len(text) * 0.5
    
    def extract_hashtags(self, texts: List[str]) -> List[str]:
        """
        ä»OCRæ–‡æœ¬ä¸­æå–æ‰€æœ‰å¸¦#çš„æ ‡ç­¾
        
        Args:
            texts: OCRè¯†åˆ«å‡ºçš„æ–‡æœ¬åˆ—è¡¨
            
        Returns:
            æå–å‡ºçš„æ ‡ç­¾åˆ—è¡¨ï¼ˆä¸å«#ç¬¦å·ï¼‰
        """
        hashtags = []
        full_text = ' '.join(texts)
        
        # æ–¹æ³•1: æ­£åˆ™åŒ¹é… #æ ‡ç­¾
        # åŒ¹é… #åé¢è·Ÿç€çš„ä¸­è‹±æ–‡å­—ç¬¦
        pattern = r'#([^\s#@ï¼Œã€‚ï¼ï¼Ÿã€ï¼šï¼›""''ã€ã€‘ã€Šã€‹\[\]]+)'
        matches = re.findall(pattern, full_text)
        
        for tag in matches:
            tag = tag.strip()
            if tag and len(tag) >= 2:
                hashtags.append(tag)
        
        # æ–¹æ³•2: å¦‚æœæ²¡æ‰¾åˆ°#ï¼Œå°è¯•ç”¨ç©ºæ ¼åˆ†å‰²æ‰¾å¸¦#çš„éƒ¨åˆ†
        if not hashtags:
            for text in texts:
                if '#' in text:
                    # æŒ‰#åˆ†å‰²
                    parts = text.split('#')
                    for part in parts:
                        part = part.strip()
                        # è¿›ä¸€æ­¥æŒ‰ç©ºæ ¼åˆ†å‰²ï¼Œå–ç¬¬ä¸€ä¸ªè¯
                        if part:
                            word = part.split()[0] if ' ' in part else part
                            word = word.strip('ï¼Œã€‚ï¼ï¼Ÿã€ï¼šï¼›')
                            if word and len(word) >= 2:
                                hashtags.append(word)
        
        # å»é‡
        unique_hashtags = list(dict.fromkeys(hashtags))
        
        if unique_hashtags:
            logger.info(f"ğŸ·ï¸ æå–åˆ° {len(unique_hashtags)} ä¸ªæ ‡ç­¾:")
            for tag in unique_hashtags:
                logger.info(f"  #{tag}")
        
        return unique_hashtags
    
    def extract_game_from_hashtags(self, hashtags: List[str]) -> Optional[str]:
        """
        ä»æ ‡ç­¾åˆ—è¡¨ä¸­æå–æ¸¸æˆåç§°
        è§„åˆ™ï¼šå»æ‰"ä¸‹è½½"ã€"å®‰è£…"ã€"æ”»ç•¥"ç­‰åç¼€ï¼Œæ‰¾åˆ°æœ€çŸ­çš„åŸºç¡€åç§°
        
        Args:
            hashtags: æ ‡ç­¾åˆ—è¡¨
            
        Returns:
            æ¸¸æˆåç§°
        """
        if not hashtags:
            return None
        
        # åç¼€æ¸…ç†åˆ—è¡¨ï¼ˆæŒ‰é•¿åº¦æ’åºï¼Œå…ˆåŒ¹é…é•¿çš„ï¼‰
        suffixes = [
            'æ€ä¹ˆä¸‹è½½', 'å®‰å“ä¸‹è½½', 'è‹¹æœä¸‹è½½', 'iosä¸‹è½½',
            'æ‰‹æœºç‰ˆä¸‹è½½', 'ç”µè„‘ç‰ˆä¸‹è½½', 'æœ€æ–°ç‰ˆä¸‹è½½',
            'ä¸‹è½½å®‰è£…', 'å®‰è£…æ•™ç¨‹', 'ä¸‹è½½æ•™ç¨‹',
            'ä¸‹è½½', 'å®‰è£…', 'æ”»ç•¥', 'ç¤¼åŒ…', 'ç¤¼åŒ…ç ',
            'æ‰‹æœºç‰ˆ', 'ç”µè„‘ç‰ˆ', 'å®‰å“ç‰ˆ', 'iosç‰ˆ',
            'å®˜æ–¹ç‰ˆ', 'æ­£ç‰ˆ', 'ç ´è§£ç‰ˆ', 'æ±‰åŒ–ç‰ˆ',
            'æœ€æ–°ç‰ˆ', 'è€ç‰ˆæœ¬', 'æ–°ç‰ˆæœ¬',
        ]
        
        # æ¸…ç†æ¯ä¸ªæ ‡ç­¾ï¼Œæå–åŸºç¡€åç§°
        base_names = []
        for tag in hashtags:
            name = tag
            for suffix in suffixes:
                if name.endswith(suffix):
                    name = name[:-len(suffix)]
                    break
            
            name = name.strip()
            if name and len(name) >= 2:
                base_names.append(name)
        
        if not base_names:
            return None
        
        # æ‰¾å‡ºç°æ¬¡æ•°æœ€å¤šçš„åŸºç¡€åç§°
        from collections import Counter
        name_counts = Counter(base_names)
        most_common = name_counts.most_common(1)
        
        if most_common:
            game_name = most_common[0][0]
            logger.info(f"ğŸ® è¯†åˆ«æ¸¸æˆåç§°: {game_name}")
            return game_name
        
        # å¦‚æœéƒ½åªå‡ºç°ä¸€æ¬¡ï¼Œè¿”å›æœ€çŸ­çš„
        return min(base_names, key=len)
    
    def extract_game_names(self, texts: List[str]) -> List[Dict[str, Any]]:
        """
        ä»OCRæ–‡æœ¬ä¸­æå–æ¸¸æˆåç§°
        æ–°ç­–ç•¥ï¼šåªæå–å¸¦#çš„æ ‡ç­¾ï¼Œä»æ ‡ç­¾ä¸­è¯†åˆ«æ¸¸æˆå
        
        Args:
            texts: OCRè¯†åˆ«å‡ºçš„æ–‡æœ¬åˆ—è¡¨
            
        Returns:
            æå–å‡ºçš„æ¸¸æˆä¿¡æ¯åˆ—è¡¨
        """
        games = []
        
        # ç¬¬ä¸€æ­¥ï¼šæå–æ‰€æœ‰#æ ‡ç­¾
        hashtags = self.extract_hashtags(texts)
        
        if not hashtags:
            logger.warning("æœªæ‰¾åˆ°ä»»ä½•#æ ‡ç­¾")
            return games
        
        # ç¬¬äºŒæ­¥ï¼šä»æ ‡ç­¾ä¸­æå–æ¸¸æˆå
        game_name = self.extract_game_from_hashtags(hashtags)
        
        if game_name:
            games.append({
                "name": game_name,
                "original_text": ' '.join([f'#{t}' for t in hashtags]),
                "hashtags": hashtags,
                "score": len(hashtags),  # æ ‡ç­¾è¶Šå¤šï¼Œç½®ä¿¡åº¦è¶Šé«˜
                "matched_keywords": ["hashtag_extract"]
            })
        
        return games
    
    def _extract_game_name_from_text(self, text: str) -> Optional[str]:
        """ä»æ–‡æœ¬ä¸­æå–æ¸¸æˆåç§°"""
        # ç§»é™¤å¸¸è§åç¼€
        suffixes_to_remove = [
            'å®˜æ–¹ç‰ˆ', 'æ­£ç‰ˆ', 'æ‰‹æ¸¸', 'æ‰‹æœºç‰ˆ', 'æœ€æ–°ç‰ˆ', 'ä¸­æ–‡ç‰ˆ',
            'æ±‰åŒ–ç‰ˆ', 'å®‰å“ç‰ˆ', 'ä¸‹è½½', 'å®‰è£…åŒ…', 'apk', 'APK',
            'ç ´è§£ç‰ˆ', 'æ— é™', 'å…è´¹', 'ç¤¼åŒ…ç ', 'æ”»ç•¥',
        ]
        
        result = text
        for suffix in suffixes_to_remove:
            if result.endswith(suffix):
                result = result[:-len(suffix)]
        
        # ç§»é™¤å¸¸è§å‰ç¼€
        prefixes_to_remove = ['ä¸‹è½½', 'æ¨è', 'çƒ­é—¨', 'æœ€æ–°', 'å…è´¹']
        for prefix in prefixes_to_remove:
            if result.startswith(prefix):
                result = result[len(prefix):]
        
        result = result.strip()
        
        # éªŒè¯ç»“æœ
        if len(result) >= 2 and len(result) <= 20:
            return result
        
        return None
    
    def _clean_game_name(self, name: str) -> str:
        """æ¸…ç†æ¸¸æˆåç§°"""
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦
        name = re.sub(r'[^\w\u4e00-\u9fff\-]', '', name)
        return name.strip()
    
    def _is_likely_game_name(self, keyword: str, context: str) -> bool:
        """åˆ¤æ–­å…³é”®è¯æ˜¯å¦å¯èƒ½æ˜¯æ¸¸æˆå"""
        # æ¸¸æˆåé€šå¸¸åŒ…å«ä»¥ä¸‹ç‰¹å¾
        game_indicators = [
            'ä¼ å¥‡', 'ä»™ä¾ ', 'æ­¦ä¾ ', 'ä¸‰å›½', 'è¥¿æ¸¸', 'ä¿®ä»™', 'å¥‡è¿¹',
            'æˆ˜äº‰', 'ç­–ç•¥', 'å¡ç‰Œ', 'å†’é™©', 'é­”å¹»', 'ç¥è¯', 'å¾é€”',
            'ç‹è€…', 'éƒ¨è½', 'å¸å›½', 'ä¸–ç•Œ', 'å¤§é™†', 'ç‹å›½', 'ä¼ è¯´',
        ]
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«æ¸¸æˆæŒ‡ç¤ºè¯
        if any(indicator in keyword for indicator in game_indicators):
            return True
        
        # æ£€æŸ¥ä¸Šä¸‹æ–‡ä¸­æ˜¯å¦æœ‰æ¸¸æˆç›¸å…³è¯
        context_lower = context.lower()
        if any(kw in context_lower for kw in ['æ¸¸æˆ', 'ä¸‹è½½', 'game', 'apk', 'æ‰‹æ¸¸']):
            if keyword in context:
                return True
        
        return False
    
    def verify_game_by_search(self, text: str) -> Dict[str, Any]:
        """
        é€šè¿‡ç½‘ç»œæœç´¢éªŒè¯æ–‡æœ¬æ˜¯å¦ä¸ºæ¸¸æˆåç§°
        
        Args:
            text: å¾…éªŒè¯çš„æ–‡æœ¬
            
        Returns:
            éªŒè¯ç»“æœå­—å…¸ï¼ŒåŒ…å« is_game, confidence, game_name ç­‰
        """
        result = {
            "text": text,
            "is_game": False,
            "confidence": 0.0,
            "game_name": None,
            "search_hints": []
        }
        
        if not text or len(text) < 2:
            return result
        
        # å…ˆæ¸…ç†æ–‡æœ¬ï¼Œç§»é™¤å¸¸è§åç¼€
        clean_text = text
        suffixes = ['è€ç‰ˆæœ¬', 'æ–°ç‰ˆæœ¬', 'å®‰è£…æ•™ç¨‹', 'æ•™ç¨‹', 'æ”»ç•¥', 'ä¸‹è½½', 'å®‰è£…åŒ…', 'æ‰‹æœºç‰ˆ', 'ç”µè„‘ç‰ˆ', 'å®‰å“ç‰ˆ']
        for suffix in suffixes:
            if clean_text.endswith(suffix):
                clean_text = clean_text[:-len(suffix)]
        
        # æ’é™¤æ˜æ˜¾ä¸æ˜¯æ¸¸æˆåçš„
        not_game_patterns = ['æ•™ç¨‹', 'ç‰ˆæœ¬', 'å®‰è£…', 'ä¸‹è½½', 'æ”»ç•¥', 'ç¤¼åŒ…', 'åŠ é¢', 'æœºç‰ˆ']
        if any(p in clean_text for p in not_game_patterns) or len(clean_text) < 2:
            return result
        
        # ä½¿ç”¨æ¸…ç†åçš„æ–‡æœ¬æœç´¢
        text = clean_text if len(clean_text) >= 2 else text
        
        try:
            # æœç´¢ "xxx æ¸¸æˆä¸‹è½½"
            search_query = f"{text} æ¸¸æˆä¸‹è½½"
            url = f"https://www.baidu.com/s?wd={quote(search_query)}"
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            }
            
            response = requests.get(url, headers=headers, timeout=10)
            response.encoding = 'utf-8'
            html = response.text.lower()
            
            # åˆ†ææœç´¢ç»“æœä¸­çš„å…³é”®è¯
            game_indicators = [
                ('taptap', 3),        # TapTapæ˜¯ä¸“ä¸šæ¸¸æˆå¹³å°
                ('4399', 2),          # 4399æ¸¸æˆå¹³å°
                ('ä¹æ¸¸', 2),           # ä¹æ¸¸æ¸¸æˆå¹³å°
                ('å¥½æ¸¸å¿«çˆ†', 2),        # æ¸¸æˆèµ„è®¯å¹³å°
                ('æ‰‹æ¸¸', 1),
                ('æ‰‹æœºæ¸¸æˆ', 1),
                ('å®‰å“æ¸¸æˆ', 1),
                ('iosæ¸¸æˆ', 1),
                ('æ¸¸æˆä¸‹è½½', 1),
                ('apkä¸‹è½½', 1),
                ('æ¸¸æˆæ”»ç•¥', 1),
                ('æ¸¸æˆç¤¼åŒ…', 1),
            ]
            
            score = 0
            hints = []
            
            for indicator, weight in game_indicators:
                if indicator in html:
                    score += weight
                    hints.append(indicator)
            
            # æ£€æŸ¥æ˜¯å¦åœ¨æœç´¢ç»“æœä¸­æœ‰æ˜ç¡®çš„æ¸¸æˆç›¸å…³æè¿°
            if f'{text.lower()}æ˜¯ä¸€æ¬¾' in html or f'ã€Š{text.lower()}ã€‹' in html:
                score += 2
                hints.append('æ¸¸æˆä»‹ç»')
            
            # è®¡ç®—ç½®ä¿¡åº¦
            confidence = min(score / 10.0, 1.0)
            
            result["confidence"] = confidence
            result["search_hints"] = hints
            
            # ç½®ä¿¡åº¦é˜ˆå€¼è®¾ä¸º0.5ï¼Œç¡®ä¿å‡†ç¡®æ€§
            if confidence >= 0.5:
                result["is_game"] = True
                result["game_name"] = clean_text if len(clean_text) >= 2 else text
                logger.info(f"ğŸ® ç½‘ç»œéªŒè¯: '{result['game_name']}' ç¡®è®¤ä¸ºæ¸¸æˆ (ç½®ä¿¡åº¦: {confidence:.2f}, ä¾æ®: {hints})")
            else:
                logger.debug(f"â“ ç½‘ç»œéªŒè¯: '{text}' å¯èƒ½ä¸æ˜¯æ¸¸æˆ (ç½®ä¿¡åº¦: {confidence:.2f})")
            
            # é¿å…è¯·æ±‚è¿‡å¿«
            time.sleep(0.5)
            
        except Exception as e:
            logger.warning(f"ç½‘ç»œæœç´¢éªŒè¯å¤±è´¥: {e}")
        
        return result
    
    def verify_texts_as_games(self, texts: List[str]) -> List[Dict[str, Any]]:
        """
        æ‰¹é‡éªŒè¯æ–‡æœ¬åˆ—è¡¨ä¸­å“ªäº›æ˜¯æ¸¸æˆåç§°
        
        Args:
            texts: å¾…éªŒè¯çš„æ–‡æœ¬åˆ—è¡¨
            
        Returns:
            éªŒè¯ç»“æœåˆ—è¡¨
        """
        results = []
        verified_games = []
        
        for text in texts:
            # å…ˆç”¨æœ¬åœ°è§„åˆ™å¿«é€Ÿè¿‡æ»¤
            text = text.strip()
            if not text or len(text) < 2 or len(text) > 20:
                continue
            
            # æ’é™¤æ˜æ˜¾ä¸æ˜¯æ¸¸æˆåçš„
            if any(kw in text for kw in EXCLUDE_KEYWORDS):
                continue
            
            # æ’é™¤çº¯æ•°å­—ã€çº¯è‹±æ–‡ç­‰
            if text.isdigit() or (text.isascii() and len(text) < 4):
                continue
            
            # ç½‘ç»œæœç´¢éªŒè¯
            result = self.verify_game_by_search(text)
            results.append(result)
            
            if result["is_game"]:
                verified_games.append(result)
        
        logger.info(f"ğŸ” ç½‘ç»œéªŒè¯å®Œæˆ: {len(texts)} ä¸ªæ–‡æœ¬ä¸­æœ‰ {len(verified_games)} ä¸ªç¡®è®¤ä¸ºæ¸¸æˆ")
        return results
    
    def process_screenshot(self, image_path: Path, use_web_verify: bool = False) -> Dict[str, Any]:
        """
        å¤„ç†å•å¼ æˆªå›¾ï¼Œä»#æ ‡ç­¾ä¸­æå–æ¸¸æˆåç§°
        
        Args:
            image_path: æˆªå›¾è·¯å¾„
            use_web_verify: æ˜¯å¦ä½¿ç”¨ç½‘ç»œæœç´¢éªŒè¯æ¸¸æˆåç§°ï¼ˆé»˜è®¤å…³é—­ï¼Œå› ä¸ºæ ‡ç­¾å·²ç»å¾ˆå‡†ç¡®ï¼‰
            
        Returns:
            åŒ…å«æ ‡ç­¾å’Œè¯†åˆ«æ¸¸æˆçš„å­—å…¸
        """
        logger.info(f"ğŸ“¸ å¤„ç†æˆªå›¾: {image_path.name}")
        
        result = {
            "screenshot": image_path.name,
            "ocr_texts": [],       # æ‰€æœ‰OCRåŸå§‹æ–‡æœ¬
            "hashtags": [],        # æå–çš„#æ ‡ç­¾
            "game_name": None      # è¯†åˆ«çš„æ¸¸æˆåç§°
        }
        
        # OCRè¯†åˆ«
        texts = self.ocr_image(image_path)
        result["ocr_texts"] = texts
        
        if not texts:
            logger.warning(f"æˆªå›¾ {image_path.name} æœªè¯†åˆ«å‡ºæ–‡æœ¬")
            return result
        
        # ä»OCRç»“æœä¸­æå–#æ ‡ç­¾
        hashtags = self.extract_hashtags(texts)
        result["hashtags"] = hashtags
        
        if not hashtags:
            logger.warning(f"æˆªå›¾ {image_path.name} æœªæ‰¾åˆ°#æ ‡ç­¾")
            return result
        
        # ä»æ ‡ç­¾ä¸­æå–æ¸¸æˆå
        game_name = self.extract_game_from_hashtags(hashtags)
        result["game_name"] = game_name
        
        # ç½‘ç»œéªŒè¯ï¼ˆå¯é€‰ï¼‰
        if use_web_verify and game_name:
            verify_result = self.verify_game_by_search(game_name)
            if verify_result["is_game"]:
                result["verified"] = True
                result["confidence"] = verify_result["confidence"]
        
        # æ›´æ–°å·²è¯†åˆ«æ¸¸æˆé›†åˆ
        if game_name:
            self.recognized_games.add(game_name)
            logger.success(f"âœ… è¯†åˆ«æ¸¸æˆ: {game_name} (æ¥è‡ªæ ‡ç­¾: {hashtags})")
        
        return result
    
    def process_multiple_screenshots(self, image_paths: List[Path]) -> List[Dict[str, Any]]:
        """
        å¤„ç†å¤šå¼ æˆªå›¾
        
        Args:
            image_paths: æˆªå›¾è·¯å¾„åˆ—è¡¨
            
        Returns:
            æ‰€æœ‰æˆªå›¾çš„å¤„ç†ç»“æœåˆ—è¡¨
        """
        all_results = []
        
        for path in image_paths:
            result = self.process_screenshot(path)
            all_results.append(result)
        
        return all_results
    
    def save_to_csv(self, results: List[Dict[str, Any]] = None):
        """
        ä¿å­˜è¯†åˆ«ç»“æœåˆ°CSVæ–‡ä»¶
        åªä¿å­˜æˆªå›¾åã€æ¸¸æˆåå’Œæ ‡ç­¾
        
        Args:
            results: process_screenshotè¿”å›çš„ç»“æœåˆ—è¡¨
        """
        import pandas as pd
        from datetime import datetime
        
        if results is None:
            # å…¼å®¹æ—§æ¨¡å¼
            results = [{"game_name": name} for name in self.recognized_games]
        
        if not results:
            logger.warning("æ²¡æœ‰æ•°æ®å¯ä¿å­˜")
            return
        
        # è½¬æ¢ä¸ºDataFrameæ ¼å¼
        rows = []
        max_hashtags = 0
        
        for result in results:
            if isinstance(result, dict):
                hashtags = result.get('hashtags', [])
                max_hashtags = max(max_hashtags, len(hashtags))
                
                row = {
                    'screenshot': result.get('screenshot', ''),
                    'game_name': result.get('game_name', ''),
                    'hashtags': '|'.join(hashtags) if hashtags else '',
                    'created_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                }
                
                # å°†æ ‡ç­¾åˆ†åˆ—å­˜å‚¨
                for i, tag in enumerate(hashtags):
                    row[f'tag_{i+1}'] = tag
                
                rows.append(row)
        
        if not rows:
            logger.warning("æ²¡æœ‰æœ‰æ•ˆæ•°æ®å¯ä¿å­˜")
            return
        
        df = pd.DataFrame(rows)
        
        # é‡æ–°æ’åˆ—åˆ—é¡ºåº
        cols = ['screenshot', 'game_name', 'hashtags', 'created_at']
        tag_cols = [c for c in df.columns if c.startswith('tag_')]
        tag_cols.sort(key=lambda x: int(x.split('_')[-1]))
        final_cols = [c for c in cols if c in df.columns] + tag_cols
        df = df[final_cols]
        
        # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œåˆå¹¶æ•°æ®
        if GAMES_CSV_PATH.exists():
            try:
                existing_df = pd.read_csv(GAMES_CSV_PATH)
                df = pd.concat([existing_df, df], ignore_index=True)
                # æŒ‰screenshotå»é‡ï¼Œä¿ç•™æœ€æ–°çš„
                df = df.drop_duplicates(subset=['screenshot'], keep='last')
            except Exception as e:
                logger.warning(f"è¯»å–ç°æœ‰CSVæ—¶å‡ºé”™: {e}")
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        GAMES_CSV_PATH.parent.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜åˆ°CSV
        df.to_csv(GAMES_CSV_PATH, index=False, encoding='utf-8-sig')
        logger.success(f"æ¸¸æˆæ•°æ®å·²ä¿å­˜åˆ°: {GAMES_CSV_PATH}")
        logger.info(f"å…±ä¿å­˜ {len(df)} æ¡è®°å½•")
    
    def get_all_games(self) -> List[str]:
        """è·å–æ‰€æœ‰å·²è¯†åˆ«çš„æ¸¸æˆåç§°"""
        return list(self.recognized_games)
